{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1ab05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from accelerate import Accelerator, notebook_launcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb6e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx = {(i, j): i * 8 + j + 1 for i in range(8) for j in range(8)} | {\"up\": 65, \"down\": 66, \"left\": 67, \"right\": 68}\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"A dataset class for handling sequence data.\"\"\"\n",
    "    def __init__(self, data, token_to_idx):\n",
    "        self.data = data\n",
    "        self.token_to_idx = token_to_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X_sequence, Y_sequence = self.data[idx]\n",
    "        X_indices = [self.token_to_idx[token] for token in X_sequence]\n",
    "        Y_indices = [self.token_to_idx[token] for token in Y_sequence]\n",
    "        return torch.tensor(X_indices, dtype=torch.long), torch.tensor(Y_indices, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    Xs, Ys = zip(*batch)\n",
    "    Xs_padded = pad_sequence(Xs, batch_first=True, padding_value=0)\n",
    "    Ys_padded = pad_sequence(Ys, batch_first=True, padding_value=0)\n",
    "    return Xs_padded, Ys_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e2d5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration class for GPT model parameters.\"\"\"\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_head, n_layer):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.n_layer = n_layer\n",
    "        self.embd_pdrop = 0.1\n",
    "        self.resid_pdrop = 0.1\n",
    "        self.attn_pdrop = 0.1\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        assert self.config.n_embd % self.config.n_head == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "        \n",
    "        # Key, query, and value projections\n",
    "        self.key = nn.Linear(self.config.n_embd, self.config.n_embd)\n",
    "        self.query = nn.Linear(self.config.n_embd, self.config.n_embd)\n",
    "        self.value = nn.Linear(self.config.n_embd, self.config.n_embd)\n",
    "        \n",
    "        # Dropout layers\n",
    "        self.attn_drop = nn.Dropout(self.config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(self.config.resid_pdrop)\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(self.config.n_embd, self.config.n_embd)\n",
    "        \n",
    "        # Causal mask to prevent attention to future tokens\n",
    "        self.register_buffer(\"mask\", torch.tril(torch.ones((config.block_size, config.block_size))).unsqueeze(0).unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        # Calculate query, key, values for all heads and transpose\n",
    "        k = self.key(x).view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n",
    "        q = self.query(x).view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n",
    "        v = self.value(x).view(B, T, self.config.n_head, C // self.config.n_head).transpose(1, 2)\n",
    "        \n",
    "        # Ensure mask is correctly broadcasted for the batch and heads\n",
    "        mask = self.mask[:, :, :T, :T]\n",
    "        \n",
    "        # Scaled dot product attention with causality\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(C // self.config.n_head))\n",
    "        att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = (att @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        \n",
    "        # Apply dropout and projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Input embedding stem\n",
    "        self.tok_emb = nn.Embedding(self.config.vocab_size, self.config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, self.config.block_size, self.config.n_embd))\n",
    "        self.drop = nn.Dropout(self.config.embd_pdrop)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(self.config) for _ in range(self.config.n_layer)])\n",
    "        \n",
    "        # Decoder head\n",
    "        self.ln_f = nn.LayerNorm(self.config.n_embd)\n",
    "        self.head = nn.Linear(self.config.n_embd, self.config.vocab_size, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, idx):\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, \"Input Sequence Too Long.\"\n",
    "        \n",
    "        token_embeddings = self.tok_emb(idx)\n",
    "        position_embeddings = self.pos_emb[:, :T, :]\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062c8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_idx['<pad>'] = 0  # Padding token\n",
    "\n",
    "vocab_size = 70   \n",
    "block_size = 201\n",
    "embed_size = 512\n",
    "num_heads = 8\n",
    "num_layers = 8\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2afc012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'filtered_sequence.pkl'\n",
    "with open(path, 'rb') as file:\n",
    "    processed = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f43584",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(processed)\n",
    "train_ratio = 0.8\n",
    "valid_ratio = 0.1\n",
    "\n",
    "train = processed[:int(train_ratio * d)]\n",
    "validation = processed[int(train_ratio * d):int((train_ratio + valid_ratio) * d) ]\n",
    "\n",
    "test_exact = processed[int((train_ratio + valid_ratio) * d): ]\n",
    "# test_valid = valid[int((train_ratio + valid_ratio) * d): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93ea2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SequenceDataset(train, token_to_idx)\n",
    "valid_dataset = SequenceDataset(validation, token_to_idx)\n",
    "\n",
    "test_exact_dataset = SequenceDataset(test_exact, token_to_idx)\n",
    "# test_valid_dataset = ValidDataset(test_valid, token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46560d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss_sum = 0.0\n",
    "    total_valid_positions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in valid_loader:\n",
    "            logits = model(X_batch)\n",
    "            logits = logits.view(-1, logits.size(-1))  # Shape: [batch_size * seq_length, vocab_size]\n",
    "            Y_batch = Y_batch.view(-1)  # Shape: [batch_size * seq_length]\n",
    "\n",
    "            # Assuming the padding token index is 0\n",
    "            padding_token_index = 0\n",
    "            mask = (Y_batch != padding_token_index).float()  # Create a mask for valid positions\n",
    "\n",
    "            loss = criterion(logits, Y_batch)  # Calculate loss without reduction\n",
    "            masked_loss = loss * mask  # Apply mask\n",
    "            loss_sum = masked_loss.sum()  # Sum the losses at valid positions\n",
    "            valid_positions = mask.sum()  # Count valid positions\n",
    "\n",
    "            total_loss_sum += loss_sum.item()\n",
    "            total_valid_positions += valid_positions.item()\n",
    "\n",
    "    # Calculate the average loss across all valid positions\n",
    "    average_loss = total_loss_sum / total_valid_positions if total_valid_positions > 0 else 0\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d04b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function():\n",
    "\n",
    "    epochs = 15\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0, reduction = 'none')\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    config = Config(vocab_size, block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "    model = TransformerModel(config)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "    \n",
    "    train_loader, valid_loader, model, scheduler, optimizer = accelerator.prepare(train_loader, valid_loader, model, scheduler, optimizer)\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_data = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch}\")\n",
    "\n",
    "        for X_batch, Y_batch in progress_bar:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X_batch)\n",
    "\n",
    "            logits = logits.view(-1, logits.size(-1))  # Shape: [batch_size * seq_length, vocab_size]\n",
    "            Y_batch = Y_batch.view(-1)  # Shape: [batch_size * seq_length]\n",
    "\n",
    "            padding_token_index = 0  # Assuming the padding token index is 0\n",
    "            mask = (Y_batch != padding_token_index).float()\n",
    "            \n",
    "            loss = criterion(logits, Y_batch)\n",
    "\n",
    "            masked_loss = loss * mask\n",
    "            loss_sum = masked_loss.sum()\n",
    "            valid_positions = mask.sum()\n",
    "\n",
    "            loss = loss_sum / valid_positions\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_sum.item() \n",
    "            total_data += valid_positions.item()\n",
    "\n",
    "            progress_bar.set_description(f\"Epoch {epoch}, Avg Loss: {total_loss/total_data:.4f}\")\n",
    "\n",
    "        valid_loss = validate_model(model, valid_loader, criterion)\n",
    "        print(f\"Validation Loss: {valid_loss}\")\n",
    "                \n",
    "        scheduler.step()\n",
    "        progress_bar.close()\n",
    "\n",
    "        epoch_loss.append((total_loss/total_data, valid_loss))\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            model_save_path = f\"Model_{epoch+1}.pth\"\n",
    "            accelerator.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    with open(\"Loss History.pkl\", \"wb\") as f:\n",
    "        pickle.dump(epoch_loss, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad83278-8c00-4919-b610-2a1fb330d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_launcher(training_function, num_processes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27106712-7b1a-42bc-ba52-3823f8c0fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (tok_emb): Embedding(70, 512)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): CausalSelfAttention(\n",
       "        (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "        (resid_drop): Dropout(p=0.1, inplace=False)\n",
       "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): Linear(in_features=512, out_features=70, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  \n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device: \", device)\n",
    "\n",
    "config = Config(vocab_size, block_size, n_layer=8, n_head=8, n_embd=512)\n",
    "model = TransformerModel(config)\n",
    "\n",
    "state_dict = torch.load('Model_4.pth', map_location=device)\n",
    "state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97090ac9-cf87-44ca-ad10-5e5440d3846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "padding_index = token_to_idx['<pad>']  # Assuming you have a token_to_idx mapping that includes '<pad>'\n",
    "test_exact_loader = DataLoader(test_exact_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Wrap data_loader with tqdm for a progress bar\n",
    "    for X, Y_true in tqdm(test_exact_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "        X, Y_true = X.to(device), Y_true.to(device)\n",
    "        logits = model(X)   \n",
    "\n",
    "        for i in range(Y_true.shape[1]):  # Iterate over even indices\n",
    "            logits_slice = logits[:, i, :]\n",
    "            probabilities = F.softmax(logits_slice, dim=-1)\n",
    "            Y_pred = torch.argmax(probabilities, dim=-1)\n",
    "\n",
    "            # Identify non-padded positions in Y_true at position i\n",
    "            non_padded_positions = Y_true[:, i] != padding_index\n",
    "\n",
    "            # Update correct predictions considering only non-padded positions\n",
    "            correct_predictions += ((Y_pred == Y_true[:, i]) & non_padded_positions).sum().item()\n",
    "            # Update total predictions to exclude padded positions\n",
    "            total_predictions += non_padded_positions.sum().item()\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
